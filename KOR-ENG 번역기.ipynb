{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00d78805",
   "metadata": {},
   "source": [
    "Attention Tensorflow Implementation\n",
    "\n",
    "- Tensorflow.org \"Neural machine translation with attention\" 문서를 참고했습니다. <br> https://www.tensorflow.org/text/tutorials/nmt_with_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14722271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석에 필요한 사전 라이브러리를 다운받습니다.\n",
    "\n",
    "!pip install einops\n",
    "!pip install tensorflow-text\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d385087b",
   "metadata": {},
   "source": [
    "사용 데이터셋: Ai-Hub 일상생활 및 구어체 한-영 번역 병렬 말뭉치 데이터 <br>\n",
    "https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=71265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b52ee05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json 데이터셋을 가져옵니다.\n",
    "\n",
    "import json\n",
    "\n",
    "with open(r'./aihub-data.json', encoding='utf-8') as f:\n",
    "    text = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfe43376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한-영 대화 데이터셋 크기: 1200307\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋의 크기\n",
    "\n",
    "print('한-영 대화 데이터셋 크기:', len(text['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd7a4006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sn': 'ECOAR1A00922',\n",
       " 'data_set': '일상생활및구어체',\n",
       " 'domain': '해외고객과의채팅',\n",
       " 'subdomain': '숙박,음식점',\n",
       " 'en_original': 'We will send a notification message one day before your reservation.',\n",
       " 'en': 'We will send a notification message one day before your reservation.',\n",
       " 'mt': '예약 하루 전에 알림 메시지를 보내드립니다.',\n",
       " 'ko': '예약 하루 전에 알림 메시지를 보내드려요.',\n",
       " 'source_language': 'en',\n",
       " 'target_language': 'ko',\n",
       " 'word_count_ko': 6.0,\n",
       " 'word_count_en': 11.0,\n",
       " 'word_ratio': 0.545,\n",
       " 'file_name': '해외고객과의채팅_숙박,음식점.xlsx',\n",
       " 'source': '크라우드 소싱',\n",
       " 'license': 'open',\n",
       " 'style': '구어체',\n",
       " 'included_unknown_words': False,\n",
       " 'ner': None}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예시\n",
    "\n",
    "text['data'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a311da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어문장 정규화\n",
    "\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "\n",
    "def reg_eng(sentence):\n",
    "    sentence = unidecode(sentence.lower().strip())\n",
    "    \n",
    "    sentence = re.sub(r\"([,.?!'])\", r\" \\1\", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z가-힣,.?!]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "989f3cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 문장 정규화\n",
    "\n",
    "def reg_kor(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    sentence = re.sub(r\"([,.?!])\", r\" \\1\", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[|ㄱ-ㅎ|ㅏ-ㅣ]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45cc411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json 파일 'en' 문장데이터 정규식 적용 후 array에 할당\n",
    "\n",
    "tgt_raw = np.array([reg_eng(text['data'][_]['en']) for _ in range(len(text['data']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9606a8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['we will send a notification message one day before your reservation .',\n",
       "       'thank you very much , see you this saturday .',\n",
       "       'good morning , and thank you for calling us today .',\n",
       "       'have you went to the garden ?',\n",
       "       'such a pity , the garden is so beautiful .'], dtype='<U387')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 목표 (tgt) 문장 예시\n",
    "\n",
    "tgt_raw[100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2ea0d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json 파일 'ko' 문장데이터 정규식 적용 후 array에 할당\n",
    "\n",
    "src_raw = np.array([reg_kor(text['data'][_]['ko']) for _ in range(len(text['data']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ce38ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['예약 하루 전에 알림 메시지를 보내드려요 .', '감사합니다 , 이번 주 토요일에 뵙겠습니다 .',\n",
       "       '좋은 아침이에요 , 오늘 전화해 주셔서 감사합니다 .', '정원에도 가셨나요 ?',\n",
       "       '아쉽네요 , 정원이 정말 아름답거든요 .'], dtype='<U208')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존 (src) 문장 예시\n",
    "\n",
    "src_raw[100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ceed58cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 데이터셋과 Validation 데이터셋을 나눕니다.\n",
    "# rnd_train 변수로 train : validation = 8:2 비율로 랜덤하게 데이터를 나누고,\n",
    "# 64 사이즈의 배치데이터로 나눕니다.\n",
    "\n",
    "buffer_size = len(src_raw); batch_size = 64;\n",
    "\n",
    "rnd_train = np.random.uniform(size = (len(tgt_raw), )) < 0.8\n",
    "\n",
    "train_raw = (tf.data.Dataset\n",
    "             .from_tensor_slices((src_raw[rnd_train], tgt_raw[rnd_train]))\n",
    "             .shuffle(buffer_size)\n",
    "             .batch(batch_size))\n",
    "\n",
    "validation_raw = (tf.data.Dataset\n",
    "             .from_tensor_slices((src_raw[~rnd_train], tgt_raw[~rnd_train]))\n",
    "             .shuffle(buffer_size)\n",
    "             .batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3bcd998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장의 시작과 끝을 '[START]', 'END' 변수로 명시합니다.\n",
    "\n",
    "def sos_eos_tokenize(sentence):\n",
    "    sentence = tf.strings.join(['[START]', sentence, '[END]'], separator = ' ')\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "800ff25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src 문장을 단어 단위 형태소로 벡터화합니다.\n",
    "\n",
    "max_vocab_size = 10000\n",
    "\n",
    "src_text_processor = tf.keras.layers.TextVectorization(standardize = sos_eos_tokenize,\n",
    "                                                      max_tokens = max_vocab_size,\n",
    "                                                      ragged = True)\n",
    "src_text_processor.adapt(train_raw.map(lambda src, tgt: src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "324c32e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', ',', '수', '?', '있습니다', '이']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b4249df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', ',', 'the', 'to', 'i', 'you']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tgt 문장을 단어 단위 형태소로 벡터화합니다.\n",
    "\n",
    "tgt_text_processor = tf.keras.layers.TextVectorization(standardize = sos_eos_tokenize,\n",
    "                                                       max_tokens = max_vocab_size,\n",
    "                                                       ragged=True)\n",
    "\n",
    "tgt_text_processor.adapt(train_raw.map(lambda src, tgt: tgt))\n",
    "tgt_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13853116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Validation 데이터셋을 zero-padded 토큰으로 변화합니다.\n",
    "# Attention 모델에 사용하기 위해 한 단어의 텀을 가진 tgt_in, tgt_out 변수를 분리하고,\n",
    "# tgt_in 변수와 context_vector인 src 변수를 묶어줍니다.\n",
    "\n",
    "def text_processor(src, tgt):\n",
    "    src = src_text_processor(src).to_tensor()\n",
    "    tgt = tgt_text_processor(tgt)\n",
    "    tgt_in = tgt[:, :-1].to_tensor()\n",
    "    tgt_out = tgt[:, 1:].to_tensor()\n",
    "    \n",
    "    return (src, tgt_in), tgt_out\n",
    "\n",
    "train = train_raw.map(text_processor, tf.data.AUTOTUNE)\n",
    "validation = validation_raw.map(text_processor, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95fc6849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized source sentences\n",
      "[   2    1    1    1 1837    5    1  374   19    4]\n",
      "--------------------\n",
      "vectorized target input sentences (with [START], without [END])\n",
      "[  2  16  41  36   7 779  97   1   5  11]\n",
      "--------------------\n",
      "vectorized target output sentences (without [START], with [END])\n",
      "[ 16  41  36   7 779  97   1   5  11  13]\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 예시\n",
    "\n",
    "for (ex_src_tok, ex_tgt_in), ex_tgt_out in train.take(1):\n",
    "    print('vectorized source sentences')\n",
    "    print(ex_src_tok[0, :10].numpy())\n",
    "    print('-' * 20)\n",
    "    print('vectorized target input sentences (with [START], without [END])')\n",
    "    print(ex_tgt_in[0, :10].numpy())\n",
    "    print('-' * 20)\n",
    "    print('vectorized target output sentences (without [START], with [END])')\n",
    "    print(ex_tgt_out[0, :10].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1163accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "848dc4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터의 크기를 조회해 분석 중간 오류가 발생 시 확인 가능하비낟.\n",
    "\n",
    "class ShapeChecker():\n",
    "    def __init__(self):\n",
    "        self.shapes = {}\n",
    "        \n",
    "    def __call__(self, tensor, names, broadcast = False):\n",
    "        if not tf.executing_eagerly():\n",
    "            return\n",
    "        \n",
    "        parsed = einops.parse_shape(tensor, names)\n",
    "        \n",
    "        for name, new_dim in parsed.items():\n",
    "            old_dim = self.shapes.get(name, None)\n",
    "            \n",
    "            if (broadcast and new_dim == 1):\n",
    "                continue\n",
    "                \n",
    "            if old_dim is None:\n",
    "                self.shapes[name] = new_dim\n",
    "                continue\n",
    "                \n",
    "            if new_dim != old_dim:\n",
    "                raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                                f\"    found: {new_dim}\\n\"\n",
    "                                f\"    expected: {old_dim}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "848b1067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "# 인코더의 목적: 디코더가 문장에 나올 다음 단어를 예측할 때 참고할 context_vector 만들기\n",
    "# 1. 토큰화한 문장을 임베딩한다.\n",
    "# 2. 임베딩화한 문장을 Bidirectional GRU layer으로 RNN 네트워크를 구축한다\n",
    "# 3. attention class에 해당 context_vector를 넘겨준다.\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, txt_processor, units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.txt_processor = txt_processor\n",
    "        self.vocab_size = txt_processor.vocabulary_size()\n",
    "        self.units = units\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, units, mask_zero = True)\n",
    "        \n",
    "        self.rnn = tf.keras.layers.Bidirectional(merge_mode = 'sum',\n",
    "                                                layer = tf.keras.layers.GRU(units,\n",
    "                                                                           return_sequences = True,\n",
    "                                                                           recurrent_initializer = 'glorot_uniform'))\n",
    "        \n",
    "    def call(self, x):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(x, 'batch s')\n",
    "        \n",
    "        # 문장을 임베딩화합니다.\n",
    "        x = self.embedding(x)\n",
    "        shape_checker(x, 'batch s units')\n",
    "        \n",
    "        # GRU layer으로 임베딩화한 sequence를 처리합니다.\n",
    "        x = self.rnn(x)\n",
    "        shape_checker(x, 'batch s units')\n",
    "        \n",
    "        # 새로운 embedding을 반환합니다.\n",
    "        return x\n",
    "    \n",
    "    def convert_input(self, texts):\n",
    "        texts = tf.convert_to_tensor(texts)\n",
    "        if len(texts.shape) == 0:\n",
    "            texts = tf.convert_to_tensor(texts)[tf_newaxis]\n",
    "        context = self.txt_processor(texts).to_tensor()\n",
    "        context = self(context)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90d11a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens, shape (batch, s): (64, 20)\n",
      "Encoder output, shape (batch, s, units): (64, 20, 256)\n"
     ]
    }
   ],
   "source": [
    "# 인코더 토큰, 벡터 크기\n",
    "\n",
    "encoder = Encoder(src_text_processor, UNITS)\n",
    "ex_context = encoder(ex_src_tok)\n",
    "\n",
    "print(f'Context tokens, shape (batch, s): {ex_src_tok.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {ex_context.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "536ddf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention layer은 Decoder가 예측할 때 Encoder의 정보를 확인할 수 있게 한다.\n",
    "# Attention layer은 context_vector와 query_vector의 가중 평균 데이터를 Decoder에 넘겨줍니다.\n",
    "\n",
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(key_dim = units, num_heads = 1, **kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        \n",
    "    def call(self, x, context):\n",
    "        shape_checker = ShapeChecker()\n",
    "        \n",
    "        shape_checker(x, 'batch t units')\n",
    "        shape_checker(context, 'batch s units')\n",
    "        \n",
    "        attn_output, attn_scores = self.mha(query = x,\n",
    "                                           value = context,\n",
    "                                           return_attention_scores = True)\n",
    "        \n",
    "        shape_checker(x, 'batch t units')\n",
    "        shape_checker(attn_scores, 'batch heads t s')\n",
    "        \n",
    "        attn_scores = tf.reduce_mean(attn_scores, axis = 1)\n",
    "        shape_checker(attn_scores, 'batch t s')\n",
    "        self.last_attention_weights = attn_scores\n",
    "        \n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95ab8861",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = CrossAttention(UNITS)\n",
    "\n",
    "embed = tf.keras.layers.Embedding(tgt_text_processor.vocabulary_size(),\n",
    "                                 output_dim = UNITS, mask_zero = True)\n",
    "ex_tgt_embed = embed(ex_tgt_in)\n",
    "\n",
    "result = attention_layer(ex_tgt_embed, ex_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6384ca0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context sequence, shape (batch, s, units): (64, 20, 256)\n",
      "Target sequence, shape (batch, t, units): (64, 26, 256)\n",
      "Attention result, shape (batch, t, units): (64, 26, 256)\n",
      "Attention weights, shape (batch, t, s):    (64, 26, 20)\n"
     ]
    }
   ],
   "source": [
    "# Attention layer 벡터 크기\n",
    "\n",
    "print(f'Context sequence, shape (batch, s, units): {ex_context.shape}')\n",
    "print(f'Target sequence, shape (batch, t, units): {ex_tgt_embed.shape}')\n",
    "print(f'Attention result, shape (batch, t, units): {result.shape}')\n",
    "print(f'Attention weights, shape (batch, t, s):    {attention_layer.last_attention_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa0fe22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder는 문장에서 다음에 나올 토큰을 예측합니다.\n",
    "# 1. 예측 토큰 이전까지 나온 문장을 GRU layer으로 처리합니다.\n",
    "# 2. 해당 정보를 query_vector으로 처리해 Attention layer에서 인코더의 결과값을 참고합니다.\n",
    "# 3. 위 정보들을 토대로 다음에 나올 단어를 예측합니다.\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    @classmethod\n",
    "    def add_method(cls, fun):\n",
    "        setattr(cls, fun.__name__, fun)\n",
    "        return fun\n",
    "    \n",
    "    def __init__(self, txt_processor, units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.txt_processor = txt_processor\n",
    "        self.vocab_size = txt_processor.vocabulary_size()\n",
    "        self.word_to_id = tf.keras.layers.StringLookup(vocabulary = txt_processor.get_vocabulary(),\n",
    "                                                      mask_token = '', oov_token = '[UNK]')\n",
    "        self.id_to_word = tf.keras.layers.StringLookup(vocabulary = txt_processor.get_vocabulary(),\n",
    "                                                      mask_token = '', oov_token = '[UNK]', invert = True)\n",
    "        self.start_token = self.word_to_id('[START]')\n",
    "        self.end_token = self.word_to_id('[END]')\n",
    "        \n",
    "        self.units = units\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
    "                                                  units, mask_zero = True)\n",
    "        self.rnn = tf.keras.layers.GRU(units,\n",
    "                                      return_sequences = True,\n",
    "                                      return_state = True,\n",
    "                                      recurrent_initializer = 'glorot_uniform')\n",
    "        self.attention = CrossAttention(units)\n",
    "        self.output_layer = tf.keras.layers.Dense(self.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "633dca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def call(self, context, x,\n",
    "         state = None,\n",
    "         return_state = False):  \n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(x, 'batch t')\n",
    "    shape_checker(context, 'batch s units')\n",
    "\n",
    "    x = self.embedding(x)\n",
    "    shape_checker(x, 'batch t units')\n",
    "\n",
    "    x, state = self.rnn(x, initial_state = state)\n",
    "    shape_checker(x, 'batch t units')\n",
    "\n",
    "    x = self.attention(x, context)\n",
    "    self.last_attention_weights = self.attention.last_attention_weights\n",
    "    shape_checker(x, 'batch t units')\n",
    "    shape_checker(self.last_attention_weights, 'batch t s')\n",
    "\n",
    "    logits = self.output_layer(x)\n",
    "    shape_checker(logits, 'batch t target_vocab_size')\n",
    "\n",
    "    if return_state:\n",
    "        return logits, state\n",
    "    else:\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2eb9620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder output shape: (batch, s, units) (64, 20, 256)\n",
      "input target tokens shape: (batch, t) (64, 26)\n",
      "logits shape shape: (batch, target_vocabulary_size) (64, 26, 10000)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(tgt_text_processor, UNITS)\n",
    "logits = decoder(ex_context, ex_tgt_in)\n",
    "\n",
    "print(f'encoder output shape: (batch, s, units) {ex_context.shape}')\n",
    "print(f'input target tokens shape: (batch, t) {ex_tgt_in.shape}')\n",
    "print(f'logits shape shape: (batch, target_vocabulary_size) {logits.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d90ef74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_initial_state(self, context):\n",
    "    batch_size = tf.shape(context)[0]\n",
    "    start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "    done = tf.zeros([batch_size, 1], dtype = tf.bool)\n",
    "    embedded = self.embedding(start_tokens)\n",
    "    \n",
    "    return start_tokens, done, self.rnn.get_initial_state(embedded)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2c38c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def tokens_to_text(self, tokens):\n",
    "    words = self.id_to_word(tokens)\n",
    "    result = tf.strings.reduce_join(words, axis = -1, separator = ' ')\n",
    "    result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
    "    result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9fdd76be",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
    "    logits, state = self(context, next_token,\n",
    "                        state = state, return_state = True)\n",
    "    \n",
    "    if temperature == 0.0:\n",
    "        next_token = tf.argmax(logits, axis = -1)\n",
    "    else:\n",
    "        logits = logits[:, -1, :]/temperature\n",
    "        next_token = tf.random.categorical(logits, num_samples = 1)\n",
    "        \n",
    "    done = done | (next_token == self.end_token)\n",
    "    next_token = tf.where(done, tf.constant(0, dtype = tf.int64), next_token)\n",
    "    \n",
    "    return next_token, done, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "502c5c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'alternatively whisk minus penalties inquiries correction comfortably mistakes sole zones',\n",
       "       b'tastier filming festival climb tints fight anything accurately accepted bucket',\n",
       "       b'w fifteen situations classrooms unicorn needing gaps team ring organ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token, done, state = decoder.get_initial_state(ex_context)\n",
    "tokens = []\n",
    "\n",
    "for _ in range(10):\n",
    "    next_token, done, state = decoder.get_next_token(ex_context,\n",
    "                                                    next_token,\n",
    "                                                    done, state, temperature = 1.0)\n",
    "    tokens.append(next_token)\n",
    "    \n",
    "tokens = tf.concat(tokens, axis = -1)\n",
    "\n",
    "result = decoder.tokens_to_text(tokens)\n",
    "result[:3].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c6214fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train할 번역기 모델을 구축합니다.\n",
    "\n",
    "class Translator(tf.keras.Model):\n",
    "    @classmethod\n",
    "    def add_method(cls, fun):\n",
    "        setattr(cls, fun.__name__, fun)\n",
    "        return fun\n",
    "    \n",
    "    def __init__(self, units, src_text_processor, tgt_text_processor):\n",
    "        super().__init__()\n",
    "        \n",
    "        encoder = Encoder(src_text_processor, units)\n",
    "        decoder = Decoder(tgt_text_processor, units)\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        context, x = inputs\n",
    "        context = self.encoder(context)\n",
    "        logits = self.decoder(context, x)\n",
    "        \n",
    "        try:\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "db28d393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens, shape: (batch, s, units) (64, 20)\n",
      "Target tokens, shape: (batch, t) (64, 26)\n",
      "logits, shape: (batch, t, target_vocabulary_size) (64, 26, 10000)\n"
     ]
    }
   ],
   "source": [
    "# 모델의 크기를 확인합니다.\n",
    "\n",
    "model = Translator(UNITS, src_text_processor, tgt_text_processor)\n",
    "\n",
    "logits = model((ex_src_tok, ex_tgt_in))\n",
    "\n",
    "print(f'Context tokens, shape: (batch, s, units) {ex_src_tok.shape}')\n",
    "print(f'Target tokens, shape: (batch, t) {ex_tgt_in.shape}')\n",
    "print(f'logits, shape: (batch, t, target_vocabulary_size) {logits.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3af2273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True,\n",
    "                                                           reduction = 'none')\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "    \n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "11e9d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_acc(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, axis = -1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "    \n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    \n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cbbee17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=masked_loss, \n",
    "              metrics=[masked_acc, masked_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1841135d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 [==============================] - 57s 452ms/step - loss: 5.9673 - masked_acc: 0.1390 - masked_loss: 5.9673 - val_loss: 5.1048 - val_masked_acc: 0.2348 - val_masked_loss: 5.1048\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 45s 455ms/step - loss: 4.7051 - masked_acc: 0.2708 - masked_loss: 4.7051 - val_loss: 4.5241 - val_masked_acc: 0.2929 - val_masked_loss: 4.5241\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 4.3780 - masked_acc: 0.3017 - masked_loss: 4.3780 - val_loss: 4.2386 - val_masked_acc: 0.3185 - val_masked_loss: 4.2386\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 53s 527ms/step - loss: 4.1474 - masked_acc: 0.3269 - masked_loss: 4.1474 - val_loss: 4.0692 - val_masked_acc: 0.3403 - val_masked_loss: 4.0692\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 49s 493ms/step - loss: 3.9634 - masked_acc: 0.3478 - masked_loss: 3.9634 - val_loss: 3.9166 - val_masked_acc: 0.3572 - val_masked_loss: 3.9166\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 63s 632ms/step - loss: 3.8602 - masked_acc: 0.3595 - masked_loss: 3.8602 - val_loss: 3.7335 - val_masked_acc: 0.3753 - val_masked_loss: 3.7335\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 72s 720ms/step - loss: 3.7454 - masked_acc: 0.3743 - masked_loss: 3.7454 - val_loss: 3.5959 - val_masked_acc: 0.3921 - val_masked_loss: 3.5959\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 73s 727ms/step - loss: 3.6363 - masked_acc: 0.3848 - masked_loss: 3.6363 - val_loss: 3.6381 - val_masked_acc: 0.3970 - val_masked_loss: 3.6381\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 75s 752ms/step - loss: 3.5287 - masked_acc: 0.3994 - masked_loss: 3.5287 - val_loss: 3.4794 - val_masked_acc: 0.4025 - val_masked_loss: 3.4794\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 3.4360 - masked_acc: 0.4085 - masked_loss: 3.4360 - val_loss: 3.4003 - val_masked_acc: 0.4170 - val_masked_loss: 3.4003\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 59s 589ms/step - loss: 3.3340 - masked_acc: 0.4181 - masked_loss: 3.3340 - val_loss: 3.3451 - val_masked_acc: 0.4173 - val_masked_loss: 3.3451\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 59s 587ms/step - loss: 3.3119 - masked_acc: 0.4199 - masked_loss: 3.3119 - val_loss: 3.2886 - val_masked_acc: 0.4190 - val_masked_loss: 3.2886\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 65s 648ms/step - loss: 3.2332 - masked_acc: 0.4302 - masked_loss: 3.2332 - val_loss: 3.2254 - val_masked_acc: 0.4356 - val_masked_loss: 3.2254\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 68s 685ms/step - loss: 3.2044 - masked_acc: 0.4323 - masked_loss: 3.2044 - val_loss: 3.1785 - val_masked_acc: 0.4357 - val_masked_loss: 3.1785\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 64s 639ms/step - loss: 3.1370 - masked_acc: 0.4426 - masked_loss: 3.1370 - val_loss: 3.0986 - val_masked_acc: 0.4438 - val_masked_loss: 3.0986\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 86s 859ms/step - loss: 3.0892 - masked_acc: 0.4451 - masked_loss: 3.0892 - val_loss: 3.1120 - val_masked_acc: 0.4416 - val_masked_loss: 3.1120\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 99s 988ms/step - loss: 3.0314 - masked_acc: 0.4503 - masked_loss: 3.0314 - val_loss: 2.9740 - val_masked_acc: 0.4604 - val_masked_loss: 2.9740\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 75s 752ms/step - loss: 3.0110 - masked_acc: 0.4575 - masked_loss: 3.0110 - val_loss: 2.9889 - val_masked_acc: 0.4505 - val_masked_loss: 2.9889\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 74s 737ms/step - loss: 2.9597 - masked_acc: 0.4616 - masked_loss: 2.9597 - val_loss: 2.8929 - val_masked_acc: 0.4685 - val_masked_loss: 2.8929\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 75s 753ms/step - loss: 2.9110 - masked_acc: 0.4678 - masked_loss: 2.9110 - val_loss: 2.8934 - val_masked_acc: 0.4706 - val_masked_loss: 2.8934\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 75s 747ms/step - loss: 2.9027 - masked_acc: 0.4669 - masked_loss: 2.9027 - val_loss: 2.8887 - val_masked_acc: 0.4715 - val_masked_loss: 2.8887\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 74s 742ms/step - loss: 2.8733 - masked_acc: 0.4712 - masked_loss: 2.8733 - val_loss: 2.8165 - val_masked_acc: 0.4773 - val_masked_loss: 2.8165\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 74s 735ms/step - loss: 2.8718 - masked_acc: 0.4707 - masked_loss: 2.8718 - val_loss: 2.8240 - val_masked_acc: 0.4764 - val_masked_loss: 2.8240\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 73s 731ms/step - loss: 2.8063 - masked_acc: 0.4799 - masked_loss: 2.8063 - val_loss: 2.8590 - val_masked_acc: 0.4721 - val_masked_loss: 2.8590\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 75s 755ms/step - loss: 2.7822 - masked_acc: 0.4809 - masked_loss: 2.7822 - val_loss: 2.7601 - val_masked_acc: 0.4857 - val_masked_loss: 2.7601\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 76s 755ms/step - loss: 2.8035 - masked_acc: 0.4773 - masked_loss: 2.8035 - val_loss: 2.7838 - val_masked_acc: 0.4808 - val_masked_loss: 2.7838\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 74s 738ms/step - loss: 2.7653 - masked_acc: 0.4821 - masked_loss: 2.7653 - val_loss: 2.7101 - val_masked_acc: 0.4927 - val_masked_loss: 2.7101\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 74s 738ms/step - loss: 2.7133 - masked_acc: 0.4900 - masked_loss: 2.7133 - val_loss: 2.7626 - val_masked_acc: 0.4893 - val_masked_loss: 2.7626\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 73s 732ms/step - loss: 2.7010 - masked_acc: 0.4899 - masked_loss: 2.7010 - val_loss: 2.7887 - val_masked_acc: 0.4788 - val_masked_loss: 2.7887\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 75s 749ms/step - loss: 2.6875 - masked_acc: 0.4929 - masked_loss: 2.6875 - val_loss: 2.6537 - val_masked_acc: 0.4921 - val_masked_loss: 2.6537\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 72s 718ms/step - loss: 2.6856 - masked_acc: 0.4941 - masked_loss: 2.6856 - val_loss: 2.6703 - val_masked_acc: 0.4934 - val_masked_loss: 2.6703\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 75s 745ms/step - loss: 2.6878 - masked_acc: 0.4937 - masked_loss: 2.6878 - val_loss: 2.7590 - val_masked_acc: 0.4844 - val_masked_loss: 2.7590\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 75s 754ms/step - loss: 2.6728 - masked_acc: 0.4944 - masked_loss: 2.6728 - val_loss: 2.6627 - val_masked_acc: 0.4936 - val_masked_loss: 2.6627\n"
     ]
    }
   ],
   "source": [
    "# 모델을 학습합니다.\n",
    "\n",
    "history = model.fit(\n",
    "    train.repeat(), \n",
    "    epochs = 50,\n",
    "    steps_per_epoch = 100,\n",
    "    validation_data = validation,\n",
    "    validation_steps = 20,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience = 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade163aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 모델의 loss을 확인합니다.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mylim([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m(plt\u001b[38;5;241m.\u001b[39mylim())])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# 모델의 loss을 확인합니다.\n",
    "\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0db595a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x23d7b8689a0>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델의 정확도를 확인합니다.\n",
    "\n",
    "plt.plot(history.history['masked_acc'], label='accuracy')\n",
    "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
    "\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1bc2145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 문장을 입력했을 때, 토큰화 후 임베딩하여 알맞은 번역 문장을 출력합니다.\n",
    "\n",
    "@Translator.add_method\n",
    "def translate(self, texts, *, max_length = 50, temperature = 0.0):\n",
    "    context = self.encoder.convert_input(texts)\n",
    "    batch_size = tf.shape(texts)[0]\n",
    "    \n",
    "    tokens = []\n",
    "    attention_weights = []\n",
    "    next_token, done, state = self.decoder.get_initial_state(context)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        next_token, done, state = self.decoder.get_next_token(context, next_token,\n",
    "                                                             done, state, temperature)\n",
    "        \n",
    "        tokens.append(next_token)\n",
    "        attention_weights.append(self.decoder.last_attention_weights)\n",
    "        \n",
    "        if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "            break\n",
    "            \n",
    "    tokens = tf.concat(tokens, axis = -1)\n",
    "    self.last_attention_weights = tf.concat(attention_weights, axis = 1)\n",
    "    \n",
    "    result = self.decoder.tokens_to_text(tokens)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2fb5f755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'감사합니다 , 이번 주 토요일에 뵙겠습니다 .'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_raw[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e7c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장을 입력하세요: 감사합니다, 오늘 하루.\n"
     ]
    }
   ],
   "source": [
    "inp = str(input('문장을 입력하세요: '))\n",
    "inp = reg_kor(inp)\n",
    "\n",
    "result = model.translate([inp])\n",
    "result[0].numpy().decode().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fb08e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model이 학습한 weight을 저장합니다.\n",
    "\n",
    "class Export(tf.Module):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    @tf.function(input_signature = [tf.TensorSpec(dtype = tf.string, shape = [None])])\n",
    "    \n",
    "    def translate(self, inputs):\n",
    "        return self.model.translate(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "acd529f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "export = Export(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f8b9138b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_5_layer_call_fn, embedding_5_layer_call_and_return_conditional_losses, embedding_6_layer_call_fn, embedding_6_layer_call_and_return_conditional_losses, cross_attention_3_layer_call_fn while saving (showing 5 of 32). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: kor-eng-translator\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: kor-eng-translator\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(export, 'kor-eng-translator',\n",
    "                    signatures={'serving_default': export.translate})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
